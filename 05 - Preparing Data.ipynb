{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "**Your Model is Only as Good as the Training Data**\n",
    "\n",
    "Garbage in is garbage out.  Let's avoid garbage intake.\n",
    "It's important to 'scrub' your data to get it into a clean state where it provides value to training the model for machine learning.\n",
    "\n",
    "> You will likely spend most of your time in machine learning messing around with the data\n",
    "\n",
    "Data has many problems:\n",
    "\n",
    "1. Encoding (\"foo\", \"bar\", \"foobï¿½\")\n",
    "2. Consistency (e.g. mixed falsey values: '', False, 0, 'Null')\n",
    "3. Missing values ([\"foo\", \"bar\", 2], [\"foo\", , 0])\n",
    "4. [Spurious Correlations](http://www.tylervigen.com/spurious-correlations) (e.g. consumption of margerine vs. rate of divorce)\n",
    "5. Not enough data (very common)\n",
    "6. Too _**small**_ of a data time window (using only server logs on a Tuesday won't tell you about Saturday traffic)\n",
    "7. Too _**large**_ of a data time window (sales price for a house 100 years ago would naturally be much lower than the sales price of the same house today)\n",
    "8. Anomalies (sometimes these are OK)\n",
    "\n",
    "\n",
    "**Step 1**: Formulate the question that you are trying to answer!\n",
    "\n",
    "**Step 2**: Get relevant data to answer **that** question\n",
    "\n",
    "**Step 3**: Sanitize, validate, iterate the data - Create Extract, Transform, Load (ETL) scripts that you can run repeatedly (exercise the full pipeline as quickly as possible)\n",
    "\n",
    "## Data Usually Contains Bugs/Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we have some housing price data in the form of `[square footage, sale price]`, and we notice a strange outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    [900,300000],\n",
    "    [1200,400000],\n",
    "    [980,1400000], # outlier\n",
    "    [1500,480000],\n",
    "    [1800,510000],\n",
    "    [2200,550000],\n",
    "    [2400,600000]]\n",
    ")\n",
    "df.columns = ['Square Footage','Sale Price']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it so we can quickly see the problem\n",
    "%matplotlib notebook\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Sale Price in $')\n",
    "plt.plot(df['Square Footage'].values,df['Sale Price'].values,'rx');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that there is an outlier in the data--a point that doesn't fit the pattern of the other data points and may be a case we don't want to support in our model.\n",
    "\n",
    "In this case, let's imagine that item #2, the house with a square footage of 980, which sold for $1.4M is an unusual exception--and we found in the data that this was a private sale of a famous historical home. This data isn't something we want to consider in our evaluation. If we had a very large training set, this one outlier probably wouldn't hurt our training, but if we have only a few hundred or thousand examples, it can throw us off our prediction target unnecessarily.\n",
    "\n",
    "Before running ML on your data, find the outliers and decide at what point you want to cut off and restrict the anomalies to create a \"normalized\" set of data.\n",
    "You may want to build a special model to analyse extremely different groupings of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# Let's see what it looks like if we exclude the anomaly\n",
    "df = pd.DataFrame([\n",
    "    [900,300000],\n",
    "    [1200,400000],\n",
    "    #[980,1400000],\n",
    "    [1500,480000],\n",
    "    [1800,510000],\n",
    "    [2200,550000],\n",
    "    [2400,600000]]\n",
    ")\n",
    "df.columns = ['Square Footage','Sale Price']\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Sale Price in $')\n",
    "plt.plot(df['Square Footage'].values,df['Sale Price'].values,'rx');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data into Train and Test Sets\n",
    "You will need to segment your data into a training set (for teaching the model) and a test/eval set for evaluating the model. Both of these datasets need to have all of the truth data you are trying to predict.\n",
    "Make sure to hold out some of your truth data for evaluation.\n",
    "> evaluations using the same data as the training data will not give an accurate report of the efficacy of your model (you may be over or under fitting the data but the eval step will fail to give a bad score)\n",
    "\n",
    "Some libraries like scikit-learn have utilities for helping to split and manage your data.\n",
    "\n",
    "In the next example, we'll use [sklearn.model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split our data.\n",
    "\n",
    "## Validation Set\n",
    "\n",
    "Sometimes you will want to split the data 3 ways in order to have a validation set for hyperparameter tuning. Tuning the parameters of the model makes the validation data have some (small) impact on the learning of the model--this means you will need a different test hold-out dataset for final testing. \n",
    "\n",
    "> The \"test\" dataset will never be used to alter your model\n",
    "\n",
    "| standard  | hyperparameter tuning  |\n",
    "|---|---|\n",
    "| <img src=\"images/data_split_1.png\" style=\"float:left\" />  |  <img src=\"images/data_split_2.png\" style=\"float:left\" /> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up\n",
    "\n",
    "[Regression Examples](06%20-%20Regression%20Examples.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
